{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Written By   :` Mazi Boustani\n",
    "- `Date         :` 08/05/2021\n",
    "- `Purpose      :` Step by step introduction into RNN and LSTM followed by Pytorch example\n",
    "\n",
    "All images captured from: https://youtu.be/qjrad0V0uJE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential data are kind of data were the order of the data matters.\n",
    "\n",
    "Whenever the points in the dataset are dependent on the other points in the dataset.\n",
    "\n",
    "Examples:\n",
    "    - Time serices data: stock market, weather\n",
    "    - Language data: such as words in a sentence\n",
    "    - Boiological data: such as DNA sequence \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Sequence Modeling Applications](images/sequence_modeling_applications.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem with NN and sequence data\n",
    "\n",
    "Neural networks and deep neural networks are not good at sequence data as their architecture cannot preserve the sequence of data.\n",
    "\n",
    "__1- They have a fixed input length.__ That means all data being feeded into NN has to be always same size (like number of pixels in a photo). For examples, sentences with different number of words cannot be feeded into NN.\n",
    "\n",
    "__2- They can not remember the sequence of the data.__ NN can not remember the order of data. NN would deal with these two data points as same \"it is cold, not hot\" and \"it is hot, not cold\".\n",
    "\n",
    "__3- Can not share parameters across the sequence.__ So for example in sentence \"What is your car? My car is Benz\" NN should be able to tell how many times \"car\" repeated in a sentence but it does not recognize this type of pattern\n",
    "\n",
    " So we need Recurrent Neural Networks (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNs are designed in a way to fix some of problems that traditional neural networks have such as three problems above. \n",
    "\n",
    "The difference between simple NN and RNN is that simple NN is unidirectional, which means it has only one direction.\n",
    "But RNN, has loops inside it to persist the information over time and preserves the information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Feed-Forward_Network](images/Feed-Forward_Network.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![handeling_individual_time_steps](images/handeling_individual_time_steps.png )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Neurons_with_Recurrence](images/Neurons_with_Recurrence.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNN_State_Update_and_Output](images/RNN_State_Update_and_Output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN is training three weight matrices __Wxh__, __Whh__ and __Why__. These three matrices are same for each data point and only being updated though backpropagation process.\n",
    "\n",
    "\n",
    "__Wxh:__ is the weight matrix that is applied at every timestamp to the input value\n",
    "\n",
    "__Whh:__ is the weight matrix by which you update the previous state.\n",
    "\n",
    "__Why:__ is the weight matrix that is applied at every timestamp to hidden state.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Standard RNN](images/Standard RNN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In RNN Since errors are calculated and are backpropagated at each timestamp it is called `Backpropagation Through Time`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNN_Backprop_through_time](images/RNN_Backprop_through_time.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the gradients require a lot of factors of __Whh__ plus repeated gradient computations, which makes two set of problems. The 2 problems are in RNN, which LSTM helps solve: \n",
    "\n",
    "__1- Exploding Gradients:__ when many of the values, that are involved in the repeated gradient computations (such as weight matrix, or gradient themselves), are greater than 1. In this problem the gradients become extremely large, and it is very hard to optimize them. \n",
    "\n",
    "This problem can be solved via a process known as __Gradient Clipping__, which essentially scales back the gradient to smaller values.\n",
    "\n",
    "\n",
    "__2- Vanishing Gradients:__ when many of the values that are involved in the repeated gradient computations (such as weight matrix, or gradient themselves) are too small or less than 1. In this problem the gradients become smaller and smaller as these computations occur repeatedly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Standard_RNN_Gradient_Flow_Vanishing](images/Standard_RNN_Gradient_Flow_Vanishing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution #1: Activation Function__\n",
    "Using ReLU instead of `tanh`. ReLu's derivate for values above 0 is always 1.\n",
    "\n",
    "__Solution #2: Parameter Initialization__\n",
    "- initialize weights to identity matrix\n",
    "- initialize biases to zero\n",
    "\n",
    "These solutions help prevent the weights from shrinking to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "\n",
    "LSTMs are a special type Recurrent Neural Networks (RNNs). LSTM solves the vanishing gradients issue that RNN has. \n",
    "\n",
    "LSTMs are best suited for long term dependencies, it can hold more memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LSTM_1](images/LSTM_1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LSTM](images/LSTM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key building block behind LSTM is a structure known as gates. Information is added or removed through these gate.\n",
    "\n",
    "Because sigmoid function is force any value to be between 0-1 it acts like a gate, 0 means nothing or 1 means everything.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LSTM_Gate](images/LSTM_Gate.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM steps:\n",
    "\n",
    "__Forget:__ To decide which information the LSTM should keep or carry, and which information it should throw away. \n",
    "\n",
    "__Store:__ What part of new and old information is important to store in new state\n",
    "\n",
    "__Update:__ To update the second cell state `ct`\n",
    "\n",
    "__Ouput:__ To control what information send to next step `ht` and output `Yt`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LSTM_forget](images/LSTM_forget.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LSTM_store](images/LSTM_store.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LSTM_update](images/LSTM_update.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LSTM_output](images/LSTM_output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we use Starbuckss stock market price. The goal is to predict the Volume of Starbucks' stock price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c9750af771bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/preprocessing/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \"\"\"\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_function_transformer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFunctionTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBinarizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/preprocessing/_function_transformer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseEstimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransformerMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0massert_allclose_dense_sparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/utils/testing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfunctools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwraps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0moperator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mitemgetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/scipy/io/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;31m# matfile read and write\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmatlab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mloadmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msavemat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhosmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbyteordercodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;31m# netCDF file support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/scipy/io/matlab/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Matlab file read and write utilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mloadmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msavemat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhosmat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbyteordercodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msix\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmiobase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_matfile_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocfiller\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmio4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMatFile4Reader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMatFile4Writer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmio5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMatFile5Reader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMatFile5Writer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/scipy/io/matlab/miobase.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mbyteord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmisc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdoccer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbyteordercodes\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mboc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/scipy/misc/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m            'comb', 'factorial', 'factorial2', 'factorialk', 'logsumexp']\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdoccer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwho\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_who\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_source\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# for plotting\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset\n",
    "You can get the data from this link: https://query1.finance.yahoo.com/v7/finance/download/SBUX?period1=1576063151&period2=1607685551&interval=1d&events=history&includeAdjustedClose=true\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv file using Pandas library and set the Date column as index column and parse the Date column\n",
    "data = pd.read_csv('./data/SBUX.csv', index_col='Date', parse_dates=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get familiar with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Starbucks Stock Volume\")\n",
    "plt.plot(data['Volume'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all columns except Volume and Date\n",
    "X = data[['Open', 'High', 'Low', 'Close', 'Adj Close']]\n",
    "\n",
    "# Get only Volume column as input data\n",
    "y = data['Volume']\n",
    "y = y.values.reshape(y.shape[0], 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When numerical input variables are scaled to a standard range machine learning algorithms perform better.\n",
    "\n",
    "The two most popular techniques for __scaling__ numerical data prior to modeling are __normalization__ and __standardization__.\n",
    "\n",
    "__Normalization__ also helps gradient descent to converge faster. Subtract each value by min and divide by range value. Values will be between 0 and 1. (MinMaxScalar)\n",
    "\n",
    "`y = (x – min) / (max – min)`\n",
    "\n",
    "__Standardizing__ a dataset involves rescaling the distribution of values so that the mean of observed values is 0 and the standard deviation is 1.\n",
    "\n",
    "`y = (x – mean) / standard_deviation`\n",
    "\n",
    "Notice that it is a regression problem, so it is very beneficial to scale your outputs otherwise you will be dealing with a huge loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler() \n",
    "mm = MinMaxScaler()\n",
    "\n",
    "X_ss = ss.fit_transform(X)\n",
    "y_mm = mm.fit_transform(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data for training and test.\n",
    "# we cannot shuffle data since the this\n",
    "# is sequential data and the order is importand\n",
    "\n",
    "# 200 for train and 53 for test\n",
    "\n",
    "X_train = X_ss[:200, :]\n",
    "X_test = X_ss[200:, :]\n",
    "\n",
    "y_train = y_mm[:200, :]\n",
    "y_test = y_mm[200:, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Shape\", X_train.shape, y_train.shape)\n",
    "print(\"Testing Shape\", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all train and test numpy array to tensor format\n",
    "X_train_tensors = Variable(torch.Tensor(X_train))\n",
    "X_test_tensors = Variable(torch.Tensor(X_test))\n",
    "\n",
    "y_train_tensors = Variable(torch.Tensor(y_train))\n",
    "y_test_tensors = Variable(torch.Tensor(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping to rows, timestamps, features\n",
    "X_train_tensors_final = torch.reshape(X_train_tensors, (X_train_tensors.shape[0], 1, X_train_tensors.shape[1]))\n",
    "X_test_tensors_final = torch.reshape(X_test_tensors, (X_test_tensors.shape[0], 1, X_test_tensors.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Shape\", X_train_tensors_final.shape, y_train_tensors.shape)\n",
    "print(\"Testing Shape\", X_test_tensors_final.shape, y_test_tensors.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define LSTM Class\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, seq_length):\n",
    "        super(LSTM, self).__init__()\n",
    "        # number of layers\n",
    "        self.num_layers = num_layers\n",
    "        # input size\n",
    "        self.input_size = input_size\n",
    "        # hidden state\n",
    "        self.hidden_size = hidden_size\n",
    "        # sequence length\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        # construct LSTM from Pytorch\n",
    "        self.lstm = nn.LSTM(input_size=input_size,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=True)\n",
    "\n",
    "        # fully connected 1\n",
    "        self.fc_1 = nn.Linear(hidden_size, 128)\n",
    "        \n",
    "        # fully connected last layer\n",
    "        self.fc = nn.Linear(128, 1)\n",
    "\n",
    "        # activation function ad ReLu\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # define hidden state h\n",
    "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size))\n",
    "        # define internal state c\n",
    "        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size))\n",
    "\n",
    "        # Propagate input through LSTM\n",
    "        output, (hn, cn) = self.lstm(x, (h_0, c_0)) #lstm with input, hidden, and internal state\n",
    "        hn = hn.view(-1, self.hidden_size) #reshaping the data for dense layer next\n",
    "        \n",
    "        output = output[:, -1, :]\n",
    "        # relu\n",
    "        output = self.relu(output)\n",
    "        # first dense layer\n",
    "        output = self.fc_1(output)\n",
    "        # relu\n",
    "        output = self.relu(output)\n",
    "        # final output\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of epochs to run\n",
    "num_epochs = 1000\n",
    "\n",
    "# learning rate value\n",
    "learning_rate = 0.001\n",
    "\n",
    "# number of features ['Open', 'High', 'Low', 'Close', 'Adj Close']\n",
    "input_size = 5\n",
    "\n",
    "# number of features in hidden state\n",
    "hidden_size = 2\n",
    "\n",
    "# number of stacked lstm layers\n",
    "num_layers = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call main LSTM class\n",
    "lstm = LSTM(input_size,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            X_train_tensors_final.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set loss function as mean-squared error for regression\n",
    "loss_func = torch.nn.MSELoss()\n",
    "\n",
    "# set optimizer as Adam\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the network for each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # set gradient to 0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # forward pass\n",
    "    outputs = lstm.forward(X_train_tensors_final)\n",
    " \n",
    "    # get the loss function\n",
    "    loss = loss_func(outputs, y_train_tensors)\n",
    "    \n",
    "    # backward pass\n",
    "    loss.backward()\n",
    " \n",
    "    # improve weights(backpropagation)\n",
    "    optimizer.step()\n",
    "    \n",
    "    # for every 100 epochs show loss value.\n",
    "    if epoch % 100 == 0:\n",
    "        print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "Run predictions on the enite dataset.\n",
    "Before running prediction on entire dataset we need to bring the original dataset into the model suitable format, which can be done by using similar code as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_ss = ss.transform(data[['Open', 'High', 'Low', 'Close', 'Adj Close']]) #old transformers\n",
    "df_y_mm = mm.transform(data.iloc[:, -1:]) #old transformers\n",
    "\n",
    "df_X_ss = Variable(torch.Tensor(df_X_ss)) #converting to Tensors\n",
    "df_y_mm = Variable(torch.Tensor(df_y_mm))\n",
    "\n",
    "#reshaping the dataset\n",
    "df_X_ss = torch.reshape(df_X_ss, (df_X_ss.shape[0], 1, df_X_ss.shape[1])) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predict = lstm(df_X_ss) #forward pass\n",
    "data_predict = train_predict.data.numpy() #numpy conversion\n",
    "dataY_plot = df_y_mm.data.numpy()\n",
    "\n",
    "data_predict = mm.inverse_transform(data_predict) #reverse transformation\n",
    "dataY_plot = mm.inverse_transform(dataY_plot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "# draw the line to split train and test, data on the right are predicted (test).\n",
    "plt.axvline(x=200, c='r', linestyle='--')\n",
    "\n",
    "# actual plot\n",
    "plt.plot(dataY_plot, label='Actuall Data')\n",
    "\n",
    "# predicted plot\n",
    "plt.plot(data_predict, label='Predicted Data')\n",
    "\n",
    "plt.title('Time-Series Prediction')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source code: https://cnvrg.io/pytorch-lstm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0b2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
